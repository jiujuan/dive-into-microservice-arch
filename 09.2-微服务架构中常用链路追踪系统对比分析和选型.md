
在微服务架构中，一个请求可能涉及多个服务调用，传统的日志和监控难以提供完整的调用链视图。分布式链路追踪系统应运而生，它通过追踪请求的整个生命周期，生成完整的调用链路图，极大地提高了问题排查的效率。

## 1. Jaeger

**特性 :** 

Jaeger 是 CNCF（Cloud Native Computing Foundation）孵化项目，灵感来源于 Google Dapper 和 OpenZipkin。它提供了一套完整的分布式追踪解决方案，包括数据采集、存储、查询和可视化。Jaeger 使用 OpenTracing API 进行数据采集，支持多种编程语言，Java 是其重点支持之一。其核心组件包括 Agent、Collector、Query、UI 和 Client Libraries。

**适用场景 (Applicable Scenarios):**

- 云原生应用和容器化环境 (Kubernetes)。
- 需要 OpenTracing 兼容性的项目。
- 对数据可视化和查询功能有较高要求的团队。
- 需要支持多种语言微服务的大型分布式系统。

**架构图 Architecture**

![image](https://github.com/user-attachments/assets/7086a813-d50d-44fc-a124-7c75fe445db4)

(from: https://www.jaegertracing.io/docs/2.7/architecture/)


**性能开销 (Performance Overhead):**

- 数据采集端（Agent/Client）：通常通过字节码增强（Java Agent）或 SDK 集成，对应用性能有轻微影响，主要体现在 trace 上下文传递和 span 创建上。
- Collector：数据写入压力主要取决于收集到的 trace 量和后端存储（Cassandra/Elasticsearch）的写入性能。
- 存储：Cassandra 或 Elasticsearch 的 I/O 和 CPU 消耗。 远程存储 API 匹配的 ClickHouse 存储

**后端存储 (Storage Backends):**

（jaeger 2.7）

后端存储介绍：

Jaeger 需要一个持久化存储后端。Cassandra、Elasticsearch 和 OpenSearch 是主要支持的分布式存储后端。

对于大规模生产部署，Jaeger 团队建议使用 OpenSearch 后端而不是 Cassandra。

其它方式的存储介绍：

- 远程存储介绍：

Jaeger 支持一个基于 gRPC 的远程存储 API v2，允许通过自定义存储后端扩展 Jaeger 生态系统，这些后端不是项目直接支持的。这些存储后端可以作为远程gRPC服务器部署。

要使用远程存储作为Jaeger存储后端，使用grpc作为存储类型并指定远程gRPC服务器地址。更多信息，请参阅 [jaeger/internal/storage/v2/grpc](https://github.com/jaegertracing/jaeger/tree/v2.7.0/internal/storage/v2/grpc)。

例如一个已知的远程存储后端：由 [robbert229 提供的 PostgreSQL](https://github.com/robbert229/jaeger-postgresql)

- 归档存储 Archive Storage

Jaeger支持两种类型的跟踪存储：主存储和归档存储。

主存储用作所有录入的跟踪数据的主要存储，因此它需要一个高度可扩展的后端，并且通常与跟踪数据的短期TTL（例如两周）一起使用，以节省存储成本。
然而，有时保存某些跟踪数据更长时间可能是有用的，例如当与事件或未来的性能改进任务相关联时。归档存储用于此目的。它可以配置为更长的保留期（甚至无限期），因为不会自动将跟踪保存到归档存储，必须由用户从 Jaeger UI 手动启动保存操作。在 Jaeger v2 中，可以为主存储和归档存储角色混合和匹配不同的后端。


**优点 (Pros):**

- **OpenTracing 原生支持:** 遵循 OpenTracing 标准，具有良好的跨平台和跨语言兼容性。
- **云原生友好:** 深度集成 Kubernetes，易于部署和管理。
- **丰富的功能:** 提供强大的 UI 界面，支持服务依赖图、火焰图、追踪筛选等。
- **可插拔的存储后端:** 支持 Cassandra 和 Elasticsearch，用户可以根据需求选择。
- **社区活跃:** 作为 CNCF 项目，拥有活跃的社区支持和持续的开发。

**缺点 (Cons):**

- **部署和维护复杂性:** 相对于轻量级方案，Jaeger 的组件较多，部署和运维成本相对较高。
- **资源消耗:** 尤其在大型部署中，Collector 和存储后端可能需要较多的资源。

**数据存储:** Jaeger 的存储后端（Cassandra 或 Elasticsearch）本身提供了最终一致性（Eventual Consistency）的保证。

- **Cassandra:** 是一个 AP（Availability + Partition Tolerance）系统，牺牲了一致性来获得高可用性和分区容忍性。数据在集群中异步复制，最终达到一致。
- **Elasticsearch:** 通常被视为 CP（Consistency + Partition Tolerance）系统，但也提供了一些 AP 特性。对于追踪数据，通常更偏向于可用性，因为历史数据的不一致性影响较小。 Jaeger 本身并不直接处理事务性数据，其对追踪数据的写入通常是尽力而为的，丢失少量数据是可以接受的，因此其设计更倾向于 AP 模型。

**是否有集群 (Cluster Support):** 是。Jaeger 的 Collector 和 Query 服务都是无状态的，可以水平扩展并部署为集群。其存储后端（Cassandra 或 Elasticsearch）本身就是分布式数据库，天然支持集群化部署。

## 2. Zipkin

**功能特性:** 

Zipkin 是一个开源的分布式追踪系统，它提供追踪数据的收集、查询和可视化功能。

- **分布式追踪 (Distributed Tracing)**：收集来自服务的计时数据，以追踪跨多个应用程序的请求。它显示了分布式系统中每个组件处理请求所需的时间。
- **延迟分析 (Latency Analysis)**：通过显示追踪中每个 Span 的持续时间来查明性能问题，帮助识别瓶颈。
- **依赖关系图 (Dependency Diagram)**：自动生成服务依赖关系图，可视化服务之间的交互方式。
- **错误检测 (Error Detection)**：可以捕获并显示与 Span 相关的错误和异常，有助于故障排除。
- **查询 API 和 UI (Query API and UI)**：提供一个 Web 用户界面，用于搜索、过滤和可视化追踪，以及一个用于程序化访问的 API。
- **采样 (Sampling)**：支持各种采样策略，以控制数据收集量。
- **仪表化 (Instrumentation)**：提供多种语言（如 Java、Scala、Go、Node.js、Ruby、Python、PHP 等）的库，用于对应用程序进行仪表化。其中许多是社区贡献的。
- **B3 传播 (B3 Propagation)**：推广了 B3 HTTP 头传播格式，该格式在追踪社区中被广泛采用。

**适用场景 (Applicable Scenarios):**

- 支持多种语言的仪表化，是多语言微服务环境的良好选择。。
- 需要快速搭建分布式追踪系统的团队，支持 docker 快速部署，也支持 java 源码部署。
- 已有 Spring Cloud Sleuth 生态的项目（Sleuth 原生支持 Zipkin）。
- 中小型微服务项目，对功能要求相对简单，更注重轻量级和易用性。
- 其更简单的数据模型和成熟的 B3 传播使其更容易与一些旧系统或自定义仪表化进行集成。

**架构图 Architecture**

zipkin 由 4 个组件构成：

- collector
- storage
- search
- web UI


数据流转传输图：

![image](https://github.com/user-attachments/assets/50b5dfba-a849-4375-98a5-c08dda2954f1)

Tracers 追踪器存在于您的应用程序中，并记录发生的操作的时间和元数据。

例如，一个配置了追踪器的网络服务器会记录它接收请求和发送响应的时间。收集到的追踪数据被称为跨度 Span。

为了追踪数据安全和最小化开销，在同一个通道内传输数据只带上 IDs，告知接收者跟踪的数据。
例如：追踪一个 HTTP 操作时，会在 headers 头部信息中添加一个传播的 ID，头部信息不用在发送诸如操作名之类的详细信息。


下面是一个用户代码调用资源 /foo 的 http 跟踪示例序列图。这个结果数据发送于单个跨度，在用户代码接收到 http 响应后异步发送到 Zipkin。

```java
┌─────────────┐ ┌───────────────────────┐  ┌─────────────┐  ┌──────────────────┐
│ User Code   │ │ Trace Instrumentation │  │ Http Client │  │ Zipkin Collector │
└─────────────┘ └───────────────────────┘  └─────────────┘  └──────────────────┘
       │                 │                         │                  │
           ┌─────────┐
       │ ──┤GET /foo ├─▶ │ ────┐                   │                 │
           └─────────┘         │ record tags
       │                 │ ◀───┘                   │                 │
                           ────┐
       │                 │     │ add trace headers │                  │
                           ◀───┘
       │                 │ ────┐                   │                  │
                               │ record timestamp
       │                 │ ◀───┘                   │                 │
                             ┌─────────────────┐
       │                 │ ──┤GET /foo         ├─▶ │                 │
                             │X-B3-TraceId: aa │     ────┐
       │                 │   │X-B3-SpanId: 6b  │   │     │           │
                             └─────────────────┘         │ invoke
       │                 │                         │     │ request   │
                                                         │
       │                 │                         │     │           │
                                 ┌────────┐          ◀───┘
       │                 │ ◀─────┤200 OK  ├─────── │                 │
                           ────┐ └────────┘
       │                 │     │ record duration   │                 │
            ┌────────┐     ◀───┘
       │ ◀──┤200 OK  ├── │                         │                 │
            └────────┘       ┌────────────────────────────────┐
       │                 │ ──┤ asynchronously report span     ├────▶ │
                             │                                │
                             │{                               │
                             │  "traceId": "aa",              │
                             │  "id": "6b",                   │
                             │  "name": "get",                │
                             │  "timestamp": 1483945573944000,│
                             │  "duration": 386000,           │
                             │  "annotations": [              │
                             │--snip--                        │
                             └────────────────────────────────┘

```

instrumentation library 列表：https://zipkin.io/pages/tracers_instrumentation 。
比如用于 java 的 [brave](https://github.com/openzipkin/brave)，Brave 是一个分布式跟踪数据流转库（instrumentation library），用于捕获和向 Zipkin 报告分布式操作延迟信息的库。Brave通常拦截生产请求以收集时间数据，关联和传播跟踪上下文。虽然通常跟踪数据会发送到 Zipkin 服务器，但第三方插件也可将数据发送给其他服务，例如 Amazon X-Ray。

Brave的使用：
大多数用户不会直接使用Brave，而是使用代表他们使用Brave的库或框架，https://github.com/openzipkin/brave/tree/master/brave。

此模块包括创建和连接表示分布式工作延迟的追踪器的功能。它还包括用于在网络边界上传播追踪上下文的库，例如，通过HTTP头信息。

一个示例设置，它通过HTTP将跟踪数据（Spans 跨度）发送到Zipkin（而不是Kafka）：

```java
// Configure a reporter, which controls how often spans are sent
//   (this dependency is io.zipkin.reporter2:zipkin-sender-okhttp3)
sender = OkHttpSender.create("http://127.0.0.1:9411/api/v2/spans");
//   (this dependency is io.zipkin.reporter2:zipkin-reporter-brave)
zipkinSpanHandler = AsyncZipkinSpanHandler.create(sender);

// Create a tracing component with the service name you want to see in Zipkin.
tracing = Tracing.newBuilder()
                 .localServiceName("my-service")
                 .addSpanHandler(zipkinSpanHandler)
                 .build();

// Tracing exposes objects you might need, most importantly the tracer
tracer = tracing.tracer();

// Failing to close resources can result in dropped spans! When tracing is no
// longer needed, close the components you made in reverse order. This might be
// a shutdown hook for some users.
tracing.close();
zipkinSpanHandler.close();
sender.close();
```
更多 brave 用法请看这里：[https://github.com/openzipkin/brave/tree/master/brave](https://github.com/openzipkin/brave/tree/master/brave)


**性能开销 (Performance Overhead):**

- **Agent/仪表化开销**: 通常被认为是轻量级的。开销通常在个位数百分比 (例如，2-5% 的 CPU 开销)，但它会因仪表化的深度和应用程序工作负载而异。

- **后端开销**: zipkin-server 本身相对轻量。主要的性能因素在于选择的存储后端。Elasticsearch 或 Cassandra 在适当扩展时可以处理大量数据，但需要足够的资源。

**优点 (Pros):**

- **部署简单:** Zipkin Server 可以以单个 JAR 包运行，部署和运维门槛低。
- **易于集成:** 与 Spring Cloud 生态系统深度融合，Spring Cloud Sleuth 提供了开箱即用的支持。
- **轻量级:** 对于小型或中型项目，Zipkin 的资源消耗相对较低。
- **多种存储选择:** 提供了灵活的存储后端存储选择，支持内存存储、MySQL、Elasticsearch、Cassandra 等多种后端。

**缺点 (Cons):**

- **功能相对基础:** 相比 Jaeger，Zipkin 的 UI 和高级功能（如服务依赖图的自动生成）可能略显逊色。
- **社区活跃度:** 相比 Jaeger 和 SkyWalking，Zipkin 的社区活跃度可能稍低，但仍然保持更新。
- **在大规模集群下性能有限**：单点写入可能成为瓶颈，集群部署需要额外配置负载均衡。
- **可视化和分析能力有限**：高级筛选和聚合等功能较为基础。

**数据存储:** 

Zipkin 的数据一致性取决于其选择的后端存储。

- **In-memory 内存:** 完全的 CP，这是一种用于测试和开发环境的快速、简单的存储方式，数据不会持久化。

- **MySQL:** 是一个 CP 系统，一种关系型数据库，适合需要持久化存储且对数据一致性要求较高的场景。

- **Elasticsearch** AP 系统，提供最终一致性；在分布式环境下，牺牲部分一致性以保证高可用。一种分布式、可扩展的搜索和分析引擎，适合需要快速检索和分析大量追踪数据的情况。

- **Cassandra**:一种高度可扩展的NoSQL数据库，适合需要处理大量写入和高可用性的场景


> 简而言之，Zipkin 适合中小型项目或需要迅速集成分布式追踪的 Spring Cloud 体系，部署和集成简单，但在功能深度和大规模集群支持方面不如 Jaeger 或 SkyWalking。其数据一致性原则随后端存储的特性而变化，通常偏向于高可用性和最终一致性。

**是否有集群 (Cluster Support):** 

- **收集器集群**：可以并行运行多个 zipkin-server 实例，通过负载均衡器分发传入的追踪数据。这为数据摄取提供了水平扩展能力。
- **存储层集群**：Zipkin 数据存储的可扩展性和高可用性完全取决于所选的后端。
    - **Elasticsearch**: 通过向 Elasticsearch 集群添加更多节点即可轻松扩展。提供复制和分片。
    - **Cassandra**: 一种分布式 NoSQL 数据库，专为高可用性和线性可扩展性而设计。
    - **MySQL**: 由于其传统的RDBMS架构，不太适合大规模追踪，但可以通过复制等技术进行集群以实现高可用性。

## 3. SkyWalking

**特性 :** 

SkyWalking 是一个开源的 APM（Application Performance Monitoring）系统，特别专注于微服务、云原生和容器化架构。它提供分布式追踪、服务网格遥测分析、度量聚合和告警等功能。SkyWalking 使用自己的探针技术（Java Agent）进行无侵入式数据采集，并支持 OpenTelemetry 和 Jaeger 协议。

**适用场景 (Applicable Scenarios):**

- 需要全方位 APM 监控的微服务架构。
- 对无侵入式数据采集有强烈需求的团队。
- 需要服务网格（Service Mesh）集成和分析的场景。
- 注重中文社区支持和文档的项目。

**性能开销 (Performance Overhead):**

- 数据采集端（Agent）：无侵入式 Java Agent，通常对应用性能影响极小，因为它是字节码增强，在运行时动态织入。
- Collector（OAP Server）：数据处理和分析的中心，CPU 和内存消耗较大，尤其在进行服务拓扑分析和告警时。
- 存储：支持 Elasticsearch、H2（用于测试）和 TiDB 等，性能取决于存储选择。

**优点 (Pros):**

- **APM 一体化:** 不仅仅是链路追踪，还集成了度量、告警、拓扑分析等 APM 功能。
- **无侵入式探针:** Java Agent 提供了非常方便的无侵入式数据采集，对业务代码零改动。
- **服务网格支持:** 对 Istio 等服务网格有很好的集成和支持。
- **丰富的监控指标:** 提供比 Zipkin 和 Jaeger 更全面的应用性能指标。
- **中文社区活跃:** 对于中文开发者来说，有非常好的社区支持和文档资源。

**缺点 (Cons):**

- **学习曲线相对陡峭:** 功能丰富也意味着配置和使用上需要一定的学习成本。
- **资源消耗:** OAP Server 和存储后端（尤其 Elasticsearch）对资源的需求较高，需要合理规划。
- **定制性相对较低:** 相对于 Jaeger 的 OpenTracing 开放性，SkyWalking 的数据格式和探针是特有的，虽然也支持其他协议。

**数据存储:** 

SkyWalking 的 OAP Server 会对采集到的数据进行聚合和分析，然后写入存储。

- **Elasticsearch:** 提供最终一致性（AP），是 SkyWalking 推荐的生产存储。
- OAP Server 在处理数据时，为了保证高吞吐和可用性，通常也倾向于 AP 模型，数据在聚合和写入过程中可能存在短暂的不一致。

**是否有集群 (Cluster Support):** 是。SkyWalking 的 OAP Server 支持集群化部署，可以通过负载均衡器实现高可用和扩展。其后端存储（如 Elasticsearch）本身就是分布式集群。

## 4. Pinpoint

**特性:** 

Pinpoint 是一个开源的 APM（Application Performance Monitoring）工具，专门为大型分布式系统设计。它提供分布式追踪、调用拓扑图、实时性能监控、代码级可见性等功能。Pinpoint 的最大特点是其**无侵入性**，通过 Java Agent 技术自动收集数据，无需修改任何业务代码。它特别擅长提供非常细粒度的代码执行信息。

**适用场景 (Applicable Scenarios):**

- 现有 Java 应用程序，特别是那些不希望修改代码以添加追踪功能的项目。
- 需要深度代码级性能分析和可见性（如方法调用耗时）的团队。
- 对实时监控和告警有较高要求的场景。
- 大型复杂的 Java 微服务系统。

**性能开销 (Performance Overhead):**

- 数据采集端（Agent）：由于其强大的无侵入性，Pinpoint Agent 对应用性能的影响非常小，但会增加 JVM 的内存消耗。它通过字节码操作在运行时动态注入，实现了对方法调用、数据库查询等操作的自动追踪。
- Collector（Collector Server）：负责收集 Agent 上报的数据，并将其存储到 HBase。数据处理量大时，Collector 需要足够的 CPU 和内存资源。
- 存储：主要使用 HBase 存储原始的追踪数据，因此 HBase 集群的性能直接影响 Pinpoint 的整体性能。

**优点 (Pros):**

- **极致无侵入性:** 无需修改一行业务代码即可实现分布式追踪和APM，部署和维护成本极低。
- **代码级可见性:** 能够追踪到方法级别的调用，并显示每个方法的执行时间，这对于定位深层性能问题非常有帮助。
- **实时监控:** 提供实时的性能指标和调用链图。
- **强大的拓扑图:** 自动生成服务调用拓扑图，直观展示服务间的依赖关系。
- **JVM 性能指标:** 除了链路追踪，还提供丰富的 JVM 性能指标监控。

**缺点 (Cons):**

- **存储依赖:** 强依赖 HBase 作为后端存储，HBase 的部署和运维相对复杂，资源消耗较高。
- **查询能力:** 查询界面相对于 Jaeger 等可能在某些高级筛选和聚合功能上略显不足。
- **社区活跃度:** 相比 SkyWalking 和 Jaeger，Pinpoint 的社区活跃度可能稍低。
- **主要面向 Java:** 虽然也有对其他语言的支持，但其核心优势和设计是围绕 Java 生态系统。

**数据存储:** Pinpoint 的存储后端 HBase 是一个 AP（Availability + Partition Tolerance）系统。它牺牲了强一致性以获得高可用性和分区容忍性。Pinpoint 对追踪数据的写入是异步进行的，通常对数据的实时性要求不是极高，因此最终一致性是可接受的。

**是否有集群 (Cluster Support):** 是。Pinpoint 的 Collector Server 可以集群化部署以实现负载均衡和高可用。其后端存储 HBase 本身就是分布式集群。

## 5. OpenTelemetry (标准化)

OpenTelemetry 不是一个具体的追踪系统软件，而是一个**可观测性数据（追踪、指标、日志）的采集、处理和导出规范及工具集**。它旨在成为行业标准，取代 OpenTracing 和 OpenCensus。

**与上述系统的关系：**

- **Jaeger** 和 **Zipkin** 都宣布原生支持或正在积极适配 OpenTelemetry 协议。这意味着你可以使用 OpenTelemetry 的 SDK 采集数据，然后将数据导出到 Jaeger 或 Zipkin 后端进行存储和分析。
- **SkyWalking** 和 **Pinpoint** 也兼容 OpenTelemetry 协议，并能够接收 OpenTelemetry 导出的数据。

**优势：**

- **供应商中立:** 避免厂商锁定，可以随意切换后端。
- **统一标准:** 简化了可观测性数据的采集，降低了集成成本。
- **未来趋势:** 随着其发展成熟，将成为主流的可观测性数据采集方案。

## 总结与对比表格

| 特性           | Jaeger                                     | Zipkin                                     | SkyWalking                           | Pinpoint                            |
| -------------- | ------------------------------------------ | ------------------------------------------ | ------------------------------------ | ----------------------------------- |
| **定位**       | 分布式追踪系统                             | 分布式追踪系统                             | APM系统（追踪+指标+日志+拓扑）       | APM系统（追踪+代码级可见性+拓扑）   |
| **数据采集**   | OpenTracing API (SDK)                      | Spring Cloud Sleuth, Zipkin API            | 无侵入式 Java Agent, 支持其他协议    | 极致无侵入式 Java Agent             |
| **云原生友好** | 高 (CNCF 项目，Kubernetes 深度集成)        | 中 (易于部署)                              | 高 (专注于云原生和容器化)            | 中 (Java应用友好)                   |
| **部署复杂度** | 中高                                       | 低 (单 JAR 包可运行)                       | 中高 (OAP Server, UI, Storage)       | 高 (强依赖 HBase)                   |
| **性能开销**   | 中 (取决于存储后端)                        | 低 (单点), 中 (集群+ES/Cassandra)          | 中高 (OAP Server 消耗较高)           | 中低 (Agent) / 高 (Collector+HBase) |
| **主要存储**   | Cassandra, Elasticsearch, OpenSearch, ClickHouse | MySQL, Elasticsearch, Cassandra, In-memory | Elasticsearch, H2, TiDB  | HBase                               |
| **功能丰富性** | 追踪可视化、服务依赖图、火焰图             | 基础追踪可视化、查找                       | 全方位 APM (追踪、指标、告警、拓扑)  | 代码级可见性、实时监控、拓扑图      |
| **社区活跃度** | 高 (CNCF 活跃项目)                         | 中                                         | 高 (特别是中文社区)                  | 中                                  |
| **CAP原则**    | 倾向于 AP (依赖存储后端)                   | 依赖存储后端                               | 倾向于 AP (依赖存储后端)             | 倾向于 AP (依赖 HBase)              |
| **集群支持**   | 是 (Collector, Query 无状态，存储后端支持) | 是 (Server 可多实例，存储后端支持)         | 是 (OAP Server 无状态，存储后端支持) | 是 (Collector 可集群，HBase 支持)   |

## 如何选择？

选择哪个链路追踪系统取决于您的具体需求和现有技术栈：

- **如果您正在构建全新的云原生应用，并且希望采用行业标准和强大的可视化功能**，**Jaeger** 是一个非常好的选择，尤其是在 Kubernetes 环境下。
- **如果您的项目主要是基于 Spring Cloud，并希望快速、简单地集成链路追踪功能**，**Zipkin** 结合 Spring Cloud Sleuth 是最便捷的方案。
- **如果您需要一个全面的 APM 解决方案，不仅限于链路追踪，还包括指标、告警、服务拓扑，并且倾向于无侵入式探针**，**SkyWalking** 将是您的首选。
- **如果您有大量的 Java 应用程序，对代码级的性能分析有强烈需求，并且可以接受 HBase 的运维复杂性**，**Pinpoint** 将是您的不二之选。
- **无论选择哪个后端系统，强烈建议您在代码层面开始采用 OpenTelemetry 规范和 SDK**。这将为您未来的技术栈升级和后端切换提供极大的灵活性，避免被特定厂商锁定。

最终，最好的选择是根据您的团队规模、技术栈、对功能复杂度的需求以及运维能力来综合考虑。

## 参考

- https://www.jaegertracing.io/docs/2.7/
- https://github.com/jaegertracing/jaeger/tree/v2.7.0/internal/storage/v2/grpc
- https://github.com/robbert229/jaeger-postgresql ，jaeber 远端存储
- https://github.com/openzipkin/brave , 兼容 Zipkin 后端服务的 Java 分布式追踪实现
- https://github.com/openzipkin/b3-propagation, b3 propagation
